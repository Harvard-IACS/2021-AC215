var tipuesearch = {"pages":[{"title":"Calendars","text":"","tags":"pages","url":"pages/calendars.html"},{"title":"FAQ","text":"General Do I have access to the video recorded materials? Yes. All AC215 / CSCIE-115 students have access to all the video captured materials. Is there a GitHub repo for the class? Yes. https://github.com/Harvard-IACS/2021-AC215 Does the individual Exercise (EX) mean I have to submit on my own but can I still work with my HW partner? You are supposed to work on your own, and you should not work with a partner. You can ask questions at Office Hours (OHs) and use all of your materials from the course up to the EX. I am an Extension School Student, can I participate in the discussion forum? And do they count towards my grade? All AC215 students - both Extension and Non-Extension- have access and should participate to the forum discussion. No grades for participating in discussion forums. I'm unable to attend lectures in person. Is attendance mandatory ? Attendance is not mandatory. All sessions are recorded, however you do miss out on interaction with Professors and other students. Auditors Can I audit this course? This course is based on frequent interactions among teaching staff and students. Given the nature of the class, our policy is not to allow auditors.","tags":"pages","url":"pages/faq.html"},{"title":"Projects","text":"Previous projects You can find some examples of the projects developed by AC295 students during the previous iterations of this course on Medium in the publication curated by the Institute of Applied Computational Science and on the page dedicated to AC295 Projects . Additional AC295 students' Final Projects can be found on Medium in Towards Data Science , the publication specialized in Data Science. Check these links: Image Segmentation Tool , Web Based Transfer Learning Portal . [TODO] Project Topic Team Milestones Deliverables","tags":"pages","url":"pages/projects.html"},{"title":"Resources","text":"GitHub Repo","tags":"pages","url":"pages/resources.html"},{"title":"Schedule","text":"Week Session (Tuesday, Thursday) Assignment (R:release - D:due) Thu 2-Sep Lecture 1: Introduction to Course/Projects - Lecture, Problem Definition, Proposed Solutions, Project Scope No assignment 7-Sep Lecture 2: Data Pipelines, Tensorflow Data, Tensorflow Records, Dask, Cloud Storage (GCS) R:EX1 14-Sep Lecture 4: Data Pipelines, Tensorflow Data, Tensorflow Records, Dask, Cloud Storage (GCS) R:EX2 - D:EX1, D:Milestone1 21-Sep Lecture 6: Computer Vision: Classification, Segmentation, Distillation and Compression D:EX2 28-Sep Lecture 9: NLP & Language Models, Transfer Learning and SOTA Models R:EX3 5-Oct Lecture 12: Virtual Environments & Virtual Machines, Containerization & Docker R:EX4 - D:EX3 12-Oct Lecture 14: Virtual Environments & Virtual Machines, Containerization & Docker R:EX5 - D:EX4 19-Oct Lecture 15: App Design, Setup & Code organization, APIs & Model serving, App frontend R:EX6 - D:EX5 26-Oct Lecture 15: App Design, Setup & Code organization, APIs & Model serving, App frontend R:EX7 - D:EX6, D:Milestone2 2-Nov Lecture 20:Google Cloud Platform (GCP), Kubernetes, Ansible Deployment R::EX8 - D:EX7 9-Nov Lecture 20:Google Cloud Platform (GCP), Kubernetes, Ansible Deployment D:EX8 16-Nov Lecture 20:Google Cloud Platform (GCP), Kubernetes, Ansible Deployment No assignment 23-Nov Lecture: Guest Lecture - Tekal AI 30-Nov Project Discussion D:Milestone3 7-Dec Final Project 14-Dec Final Project","tags":"pages","url":"pages/schedule.html"},{"title":"Syllabus","text":"TENTATIVE SYLLABUS/SCHEDULE SUBJECT TO CHANGE Welcome to APCOMP 215 / CSCIE-115: Advanced practical data science Course helpline: ac215.fall2021@gmail.com Instructor: Pavlos Protopapas Sessions: Tuesdays and Thursdays - 2:15 PM - 3:30 PM TFs: Rashmi Banthia, Shivas Jayaram, Andrew Smith, Gordon Hew Office Hours: TBD Overview of the Course This course aims to review existing Deep Learning flow while applying it to a real-world problem. Then we will build and deploy an application that uses the deep learning model to understand how to productionize models. This course follows the CS109 A/B model of balancing between concept, theory, and implementation. Split into three parts; the course starts with the review of Deep Learning concepts for data and modeling and how to apply them to different tasks, including vision and language tasks. The next part will be Development, where you use the models you trained in part 1 and incorporate them into real-world applications. Finally, you will Deploy the application in Google Cloud Platform (GCP). The three parts will cover in detail topics such as Transfer learning, Containerization using Docker, and Scaling deployments using Kubernetes. At the end of this module, you will build efficient deep learning models and design, build and deploy applications that scale. Course Topics Project Outline: Introduction to Projects Problem Definition Proposed Solutions Project Scope Deep Learning: Data Data Pipelines TensorFlow Data TensorFlow Records Dask Cloud Storage Models Computer Vision: Classification Computer Vision: Segmentation NLP & Language Models Transfer Learning and SOTA Models Distillation and Compression Development: Design Virtual Environments, Virtual Machines Containers & Docker App Design Develop Setup and Code organization APIs and Model serving App frontend Operations: Deployment, Scaling, & Automation Google Cloud Platform (GCP) Kubernetes Ansible Prerequisites Your are expected to know the following: Good working knowledge of python Good understanding on the Tensorflow Deep Learning framework Basic shell commands Programming: Experience with Python: Functions, Classes, Modules, NumPy, Pandas, Tensorflow Basic Data Structures: Dictionaries and Lists File I/O Grading Breakdown Grading Breakdown Quiz 10% Exercises 20% Milestone 1 5% Milestone 2 15% Milestone 3 20% Final Presentation & Deliverable 30% Course Components The course includes the following every week: 2 Sessions (Tue, Thu) Office hours Sessions Before the Tuesday session begins, students are expected to complete a pre-class reading assignment and and attempt a quiz based on the same. The sessions will be one and half hours and will have the following layout: Sessions will help students develop the intuition for the core concepts, provide the necessary mathematical background, and provide guidance on technical details. Sessions will be accompanied by relevant examples to clarify key concepts and techniques. Part of the session will be an instructor driven in-depth implementation walk through of code related to the topics covered. This will help you understand the concepts and give you starter code that will help in your projects. Student participation is highly encouraged in sessions. Reading Assignments The course schedule includes weekly readings which will be available before the lecture. The goal of the reading assignments is to prepare for class, to familiarize yourself with new terminology and definitions, and to determine which part of the subject needs more attention. Each Tuesday session will have a short quiz at the beginning which covers the assigned reading for that week. Homework Code that cannot be completed in class will be left as an exercise for the student to complete as homework. We will go over hints to complete your homework which will be an individual effort. Project During the entire course you will work in teams and implement a project. The various topics in the class are designed to help you build milestones in an incremental fashion and build towards the end goal. The final outcome with your project will be a fully working AI App. Project groups need to be formed by the first week of class Group size : 4 Select a project from the list given or bring your own Criteria for projects are: Deep learning task implemented should include preferably both computer vision and language models The end product should be a deployable app that uses the models in the backend in some form If you plan to bring your own project idea, the following additional criteria must be met: Dataset size 5 Gigabytes or larger Data should include preferably images and text Please find a more detailed summary of all projects here. [TODO] Deadlines All quizzes are due before the next Tuesday session. All homework are due before the next Thursday session. There will be no extensions given for the homework and quizzes. The deadline for the project milestones will be announced in the scaffold. Software All softwares required for this course and installation instructions/help will be provided in first week. Course Policies Getting Help For questions about homework, course content, package installation, after you have tried to troubleshoot yourselves, the process to get help is: Post the question on ED and hopefully, your peers will answer. Note that questions on ED are visible to everyone. The teaching staff monitors the posts. Attend Office Hours, this is the best way to get help. For private matters send an email to the instructor. Academic Honesty Ethical behavior is an important trait, from ethically handling data to the attribution of code and work of others. Thus, in AC215, we give a strong emphasis on Academic Honesty. As a student, your best guidelines are to be reasonable and fair. We have included some ideas below of acceptable and not acceptable behaviors. Engaging in not acceptable behavior regarding academic honesty will be handled accordingly. Please be responsible and when in doubt ask the course instructors. ACCEPTABLE: Discussing materials and engaging in OH. Helping debug. Using a few lines of code found online or another forum as long as you cite the origin and attribute authorship of code. Searching online to expand your knowledge and for debugging Using a tutor, provided the tutor does not do your work for you. NOT ACCEPTABLE: Accessing a solution to some problem prior to submitting your own. Failing to cite the origins of code or techniques that you discover outside of the course's own lessons and integrate into your own work. Paying or offering to pay an individual for work that you may submit as your own. Providing or making available solutions to problem sets to individuals who might take this course in the future. Searching for or soliciting outright solutions to problem sets online or elsewhere. Accommodations for students with disabilities Students needing academic adjustments or accommodations because of a documented disability must present their Faculty Letter from the Accessible Education Office (AEO) and speak with Pavlos by the end of the third week of the term: Friday, September 17. Failure to do so may result in us being unable to respond in a timely manner. All discussions will remain confidential.","tags":"pages","url":"pages/syllabus.html"},{"title":"Lecture: Guest Lecture - Tekal AI","text":"Slides Guest Lecture - Tekal AI | PDF","tags":"lectures","url":"lectures/lecture11/"},{"title":"Lecture 20:Google Cloud Platform (GCP), Kubernetes, Ansible Deployment","text":"Slides Deployment | PDF Scaling and Automation | PDF Tutorial: Mushroom App","tags":"lectures","url":"lectures/lecture10/"},{"title":"Lecture 6: Computer Vision: Classification, Segmentation, Distillation and Compression","text":"Slides Lecture 6-7-8: Computer Vision: Classification, Segmentation, Distillation and Compression| PDF Tutorials Segmentation Model Compression Mushroom App Models","tags":"lectures","url":"lectures/lecture4/"},{"title":"Lecture 9: NLP & Language Models, Transfer Learning and SOTA Models","text":"Slides Lecture 9-10-11: NLP & Language Models, Transfer Learning and SOTA Models| PDF Self Attention, Transformers, BERT| PDF Tutorials Text Classification Text Generation","tags":"lectures","url":"lectures/lecture5/"},{"title":"Lecture 12: Virtual Environments & Virtual Machines, Containerization & Docker","text":"Slides Environments| PDF Containers| PDF Containers 2 | PDF Tutorial - Mega Pipeline App 🎙️ → 📝 → 🗒️ → [🔊🇫🇷] → 🔊 In this tutorial the entire class will participate to build a Mega Pipeline App which does the following: Allows a user to Record audio using a mic The audio file is then transcribed using Google Cloud Speech to Text API The text is used as a prompt to a pre-trained GPT2 model to Generate Text (100 words) The generated text is synthesized to audio using Google Cloud Text-to-Speech API The generated text is also translated to French using googletrans The translated text is then synthesized to audio using Google Cloud Text-to-Speech API The class will work in teams: 📝Team A transcribe_audio : 🗒️Team B generate_text : 🔊Team C synthesis_audio_en : 🇫🇷Team D translate_text : 🔊Team E synthesis_audio : Each team will create a Docker container to build the required functionality. The details on what to build in each container are linked above for each team. The overall progress of this mega pipeline can be viewed here GCP Credentials File: (Download and place inside /secrets): See Ed GCS Bucket Details: input_audios - Bucket where we store the input audio files text_prompts - Bucket where we store the text prompts that was synthesized by audio to text text_paragraphs - Bucket where we store the generated text from GPT2 text_translated - Bucket where we store the translated text text_audios - Bucket where we store the audio of the paragraph of text output_audios - Bucket where we store the final French audio files","tags":"lectures","url":"lectures/lecture6/"},{"title":"Lecture 14: Virtual Environments & Virtual Machines, Containerization & Docker","text":"Slides Environments| PDF Containers| PDF Containers 2 | PDF","tags":"lectures","url":"lectures/lecture7/"},{"title":"Lecture 15: App Design, Setup & Code organization, APIs & Model serving, App frontend","text":"Slides Lecture 15: App Design, Setup & Code organization, APIs & Model serving, App frontend | PDF Lecture 16-17-18: APIs and App frontend | PDF Tutorial: Mushroom App Mushroom App - Setup & Code Organization Mushroom App - Setup GCP Credentials Mushroom App - Download Best Models Mushroom App - APIs & Frontend App GCP Credentials File:) (Download and place inside mushroom-app/secrets): (See ED)","tags":"lectures","url":"lectures/lecture8/"},{"title":"Lecture 15: App Design, Setup & Code organization, APIs & Model serving, App frontend","text":"Slides Lecture 15: App Design, Setup & Code organization, APIs & Model serving, App frontend | PDF Lecture 16-17-18: APIs and App frontend | PDF Tutorial: Mushroom App Mushroom App - Setup & Code Organization Mushroom App - Setup GCP Credentials Mushroom App - Download Best Models Mushroom App - APIs & Frontend App GCP Credentials File:) (Download and place inside mushroom-app/secrets): (See ED)","tags":"lectures","url":"lectures/lecture9/"},{"title":"Lecture 4: Data Pipelines, Tensorflow Data, Tensorflow Records, Dask, Cloud Storage (GCS)","text":"Slides Lecture 4: Data - TF Data, TF Records | PDF Lecture 5: Data - TF Data, TF Records | PDF","tags":"lectures","url":"lectures/lecture3/"},{"title":"Lecture 2: Data Pipelines, Tensorflow Data, Tensorflow Records, Dask, Cloud Storage (GCS)","text":"Slides Lecture 2: Data - Dask, Cloud Storage| PDF Lecture 3: Data - TF Data, TF Records| PDF Tutorial Week2 notebook - Dask Week3 notebook - TF Data and TF Records","tags":"lectures","url":"lectures/lecture2/"},{"title":"Lecture 1: Introduction to Course/Projects - Lecture, Problem Definition, Proposed Solutions, Project Scope","text":"Slides Lecture 1: Introduction | PDF Setup 01_tutorial_setup - Google Doc","tags":"lectures","url":"lectures/lecture1/"},{"title":"Topics in Applied Computation: Advanced Practical Data Science, MLOps","text":"Fall 2021 Pavlos Protopapas Office Hours: By appointment Course helpline: ac215.fall2021@gmail.com Welcome to AC215: Advanced practical data science, MLOps. This course aims to review existing Deep Learning flow while applying it to a real-world problem. Then we will build and deploy an application that uses the deep learning model to understand how to productionize models. This course follows the CS109 model of balancing between concept, theory, and implementation. Split into three parts; the course starts with the review of Deep Learning concepts for data and modeling and how to apply them to different tasks, including vision and language tasks. The next part will be Development, where you use the models you trained in part 1 and incorporate them into real-world applications. Finally, you will Deploy the application in Google Cloud Platform (GCP). The three parts will cover in detail topics such as Transfer learning, Containerization using Docker, and Scaling deployments using Kubernetes. At the end of this module, you will build efficient deep learning models and design, build and deploy applications that scale. Lectures: Tuesday and Thursday 2:15-3:30pm @SEC LL2.224 Note: All sessions will be streamed and video taped. Office Hours: (all times EST) (Office hours begin 09/02) TF Day Office Hours Andrew Smith Mon 7:00-8:30 PM Rashmi Banthia Tue 9:30-11:00 AM Shivas Jayaram Wed 3:30-5:00 PM Gordon Hew Sat 10:00-11:30 AM Previous Material 2020 Fall","tags":"pages","url":"pages/topics-in-applied-computation-advanced-practical-data-science-mlops/"},{"title":"Lecture 1: Notebook 1","text":"Lecture 4: Dask Dask is a useful tools 1. Add general description. Dask is a useful tools 2. Add infrastructure/platform description. Link to documentation Dask and why it is cool Dask developer community on twitter . Dask is a useful tools 3. a) What you can do you, b) what other advanced things people do. We'll go over the basics of some Dask services, but we should point out that a lot of talented people have given tutorials, check'em out. BLA BLA Table of Contents Lecture 4: Dask Part 1: The building block of scalable computing Public cloud basics Cloud pros and cons Getting started: first access to Azure Create your Azure free account Login to Azure Dashboard and create your first service Create API key to use Microsoft Azure service Script: create a set of keys for using Azure services Recap What you have learnt What you will learn next guide ADD TO LECTURE PRESENTATION <1.1 Why Dask> <1.2 Cooking with DAGs> <1.3 Scaling out, concurrency, and autorecovery> <2.1 Hello Dask: A first look at the DataFrame API> <2.2 Visualizing DAG> <2.3 Task Scheduling> <3.1 Why to use DataFrames> <3.2 Dask and Pandas: DataFrame partitioning and shuffle> <3.3 Limitations> GO TO NOTEBOOK (MAYBE ONE SUMMARY SLIDE AND JUMP INTO NOTEBOOK) <4.1 Read data, datasets, and datatypes> <4.2 Reading from relational database, HDFS, and S3> <5.1 Indexes, selecting, and dropping> <5.1.1 Filtering and Reindexing> <5.2 Joining, concatenating, and unioning> <5.2.1 Recording data> <5.3 Elementwise operations> <5.4 Dealing with missing values> <6.1 Descriptive Statistics> <6.2 Built-In aggregate functions> <6.3 Custom aggregate functions> <6.4 (Rolling window) functions> <7.1 Preare-reduce-collect-plot> <7.2 Continuous and categorial> <7.3 Density plots> <7.4 Random samples> <7.5 Heatmap> SKIP NEXT OR THIS IS THE MOST INTERESTING? SHOW CASE NOTEBOOK IN CLASS <9.1 Reading and parsing unstructured data with Bags> <9.1.1 Selecting and viewing data from a Bag> <9.1.2 Common parsing issue> <9.1.3 Delimiters> <9.2 Transforming, filtering, and folding> <9.2.1 Map, filter, and aggregate functions (foldby)> <9.2.2 Building Arrays from bags> <9.2.3 Summary stats on bags: parallel text analysis> <9.2.3.1 Bigrams> <9.2.3.2 Tokens and filtering stopwords> <9.2.3.2 Analyze bigrams> <10.1 Linear Models with Dask-ML> <10.2 Evaluating and tuning Dask-ML models> <10.3 Persisting Dask-ML models> EXERCISE: YOU LEARNED IT, NOW DO IT. REPRODUCE A PAPER FROM READING LIST <11.1 Building a Dask cluster on Amazon AWS with Docker> <11.1.1 to 11.1.7> <11.2 Running and monitoring Dask jobs on a cluster> <11.3 Cleaning up the Dask clusters on AWS> Part 1: Scalable computing If you ask different people, you'll get different answers, but one of the commonalities is that most people don't realize is that eventhough these services come with costs (i.e. both monetary and training), they provides great resources that social scientists should start exploring themself. Here are some highlights: You can use them \"anytime, anywhere\": public cloud users can access, barely always, cloud services and keep their data stored safely in the infrastructure. You won't need to plan far ahead for provisioning: public cloud users can use infinite computing and storaging resources available on demand. In this way, the user can offload some problems to the service provider such as mantaining both hardware and software. You can buy what you need, when you need it: public cloud allows you to use services eliminating any sort of up-front commitment by Cloud user. Public cloud allows teams to collaborate: Public cloud allows you to share data and collaborate more easily. Public cloud basics Cloud Computing refers to both the applications delivered as services over the Internet and the hardware/systems software in the datacenters that provide those services. The datacenter hardware and software is what we will call a Cloud . When a Cloud is made available to the public (through pay-as-you-go services), it is called a Public Cloud; the service being sold from the datacenter to the provider such as Microsoft, Google, Amazon or IBM (which might or not might be the same) called computing utility [ 1 ] and the one sold from provider to user that we will refer as a web application or more in general a service. Current examples of public computing services include AmazonWeb Services, Google AppEngine, and Microsoft Azure. A differenet deployment system from the public is the private. The private Cloud refers to internal datacenters of a business or other organization that are not made available to the public. The figure below shows the roles of the people as users or providers of different layers of the Cloud. Image adapted from Armbust et al., 2009 The National Institute of Standards and Technology (NIST) defines cloud as [ 2 ]: \"a model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction.\" Cloud computing has 5 essential characteristics: Measured Service Rapid Elasticity Broad Network Access Resource Pooling On Demand Self Service You can think about cloud computing as composition of three different areas/models: The Infrastructure as a Service ( IaaS ) is a cloud of resources provided to users. IaaS provides the basic functionality of storage and computing as service consisting in network servers, storage systems, network instrumentality, and information headquarters. The Platform as a Service ( PaaS ) is a development environment provided as a service. These advanced levels of alternative service can be designed by end-users. PaaS offers virtualized servers that multiple users work on applications or grow innovative applications, with no having to concern about keeping operating systems, server hardware, equalization or weight power calculation. The Software as a Service ( SaaS ) is an application that is offered to consumers using the Internetwork. A single case of service works in the cloud and multiple end users services. An example of Cloud Computing is Software as a Service (SaaS), where you input data on software and the data is transformed remotely through a software interface without your computer being involved. Thus SaaS eliminates client fears on storage, server applications, application development, and related common concerns of IT. In this to tutorial we are going to show you examples encompasses PaaS, when you will build the experiment using the two tutorials, and SaaS, when you will learn how to use the cloud services in the three guides. Cloud pros and cons Overall, the advantages of using a cloud outperforms the disadvantage. When deciding to build your own application, which an example in this series of guides is the experiment, it is important to consider a couple of aspects: cloud security, and cloud service accuracy. In respect to the first, we reccomend making sure that data being stored in the cloud meet the requirements of your institution/research as well as checking the warranty of the chosen provider. In respect to the second, we suggest to consider the accuracy of the algorithm provided by the service for the purpose of the research but this fall outside the purpose of this workshop. Naeem et al.[ 3 ] discusses some of the advantages and disadvantages when using cloud computing. We report them from their study in the chart below: Disadvantages Advantages Security in the Cloud Almost Unlimited Storage Technical Issues Quick Development Non-Interoperability Cost Efficient Dependency and vendor lock-in Automatic Software Integration Internet Required Shared Resources Less Reliability Easy Access to Information Less management Mobility Raised Vulnerability Better Hardware Management Prone to Attack Backup and Recovery Getting started: first access to Azure Create your Azure free account To access Azure cloud computing services, you will have to sign up for an Azure free account, if you don't already have one. If you do not have a Microsoft account either you will be asked to create one, otherwise insert your outlook account (e.g. your_email_address@outlook.com). To create your Azure account you will be asked to add your credentials as well as a credit card account. This will not be charged unless you exceed the credit provided with the one month trial version [ 4 ]. We reccomend to cancel the account after the first month in case you are not interested in the service. Follow the next steps to set up a free account: Go to https://azure.microsoft.com/en-us/ and click on free account Click on start free Create an Azure account: choose name, set password, add security info, add credit/debit card information Login to Azure Dashboard and create your first service Once you have created your Azure free account, you just need to go to the Azure portal and login using the credentials. Go to https://portal.azure.com/ and sign-in to your account to access Azure Dashboard (note: familiarize with the relevant services?) You are now ready to deploy your first public cloud service! Follow the next steps: Click on create a resource Write on the bar the name of the service you want to subscribe to. For convinience we show the Storage Account that will we use in the next guide. Type storage account in the bar and then press enter You will be directed to the a view containing a short description of the service, as well as links to the documentation and pricing information. Click on create to start deploying the services. Next, complete entering the following intormation and click on create once finished: Account name: enter lowercase and globally unique name (e.g. \"mycloudstorageplayground\") Deployment model: click on Resource manager Account kind: Storage v1 Location: East US Replication: Locally-reduntant storage (LRS) Performance: click on Standard Secure transfer required: Enabled Subscription: Free Trial Select server region: Eastus Resource Group: create a new entering name (e.g. myresourcegroup) Virtual networks: click on Enabled Pin to dashboard (optional): [x] Once the service is deployed, you will see on your Dashbord a white box with your storage account's name. Click on the box with your storage_account_name to access the storage account interface. The storage account interface shows a summary of the settings defined in the previous steps and other utilities. On the top right box you can see the region from which your service is deployied, the type of storage you have choosen as well as the type of contents you decided to store (i.e. Locally Reduntant Storage which stands for data you might use a lot and the server will know). You can also find the id of your subrscition below erased for privacy purposes. Create API key to use Microsoft Azure service We have shown you how to login to the Azure portal and how to create a Storage Account, now it is time to retrieve the key necessary to use it. We will use the key in the next guide to make requests to Microsoft Azure using Application Programming Interface (API). The API functions as an intermediary that allows two applications to talk to each other, in our case our software and Azure SaaS. The API key allows Azure to identifies your subscription account and to bill it (unless you switch your free account to a pay as you go account your account will not be billed). To retrieve your Storage Account key, start from going to the dashboard and clicking on the box with your storage_account_name . Then, click on on Access keys on the side bar. Copy storage account name and key1, clicking on the icon in the left, and paste them in the script below. Script: create a set of keys for using Azure services In the next guides, we are going to poke around with several Azure services. We reccomend you to create all the services on the list below and to save the name you will give to each service and primary key in the cell below. Here is a complete list of the services that you should create: Storage Account Face Computer Vision Bing Speech Recognition Text Analytics When looking for a service, we recommend to click on the Create a Resource button and to copy each service name on the finder bar as shown before. This will avoid you to look for service at the time, and some headache from navigating yourself through the myriad of services available. Run the cell when you are done, and a file with your key will be automatically generated and stored into the folder public_cloud_computing/guides/keys. In [ ]: ############################################################### # copy and paste your services' account name and primary key # ############################################################### # STORAGE_ACCOUNT STORAGE_ACCOUNT_NAME = '' #add your account name STORAGE_ACCOUNT_API_KEY = '' #add your account key1 # COGNITIVE_SCIENCE_FACE_ACCOUNT FACE_ACCOUNT_NAME = '' FACE_API_KEY = '' # COGNITIVE_SCIENCE_COMPUTER_VISION_ACCOUNT COMPUTER_VISION_NAME = '' COMPUTER_VISION_API_KEY = '' # SPEECH_RECOGNITION_ACCOUNT SPEECH_RECOGNITION_NAME = '' SPEECH_RECOGNITION_KEY = '' # TEXT_ANALYTICS_ACCOUNT TEXT_ANALYTICS_NAME = '' TEXT_ANALYTICS_API_KEY = '' #run this cell to write a copy of your Azure services information (NAME and API's key) #write a dictionary azure_services_keys = { 'STORAGE' : { 'NAME' : STORAGE_ACCOUNT_NAME , 'API_KEY' : STORAGE_ACCOUNT_API_KEY }, 'FACE' : { 'NAME' : FACE_ACCOUNT_NAME , 'API_KEY' : FACE_API_KEY }, 'COMPUTER_VISION' : { 'NAME' : COMPUTER_VISION_NAME , 'API_KEY' : COMPUTER_VISION_API_KEY }, 'SPEECH_RECOGNITION' : { 'NAME' : SPEECH_RECOGNITION_NAME , 'API_KEY' : SPEECH_RECOGNITION_KEY }, 'TEXT_ANALYTICS' : { 'NAME' : TEXT_ANALYTICS_NAME , 'API_KEY' : TEXT_ANALYTICS_API_KEY }} #dump the dictionary on a file and saved in the folder < /guides/keys > #import modules import pickle import json #open a .json file and copy the dictionary with all your keys with open ( \"../keys/azure_services_keys.json\" , 'wb' ) as f : pickle . dump ( azure_services_keys , f ) ################################ # run this cell once completed # ################################ Recap What you have learnt What is cloud and its advantages Access the Azure portal How to deploy public cloud service Now that you know more about cloud, what do you think about it? What you will learn next guide How to use public cloud services: What is a cloud storage Access Azure cloud storage using Storage Explorer UI and with Python SDK Create BLOB container and Upload BLOB (Big Large Binary Objects AKA image, audio, etc.) Question for you Now that you know more about cloud, what do you think about it? When would it be useful in your work, research? Footnotes [1] Armbrust et al, 2009. Above the Clouds: A Berkeley View of Cloud Computing [2] Peter Mell and Timothy Grance, 2011. The NIST Definition of Cloud Computing: recommendations of the National Institute of Standards and Technology [3] Naeem et al, 2016. Cluster Computing vs Cloud Computing: a comparison and overview [4] At subscription of a free account you will receive 200 dollars for 30 days to try pay as you go cloud services and a free account for a year. Once you exceed 200 dollars or the 30 days free trial will expired you will be asked to upgrade your subscription. In [ ]: #import library to display notebook as HTML import os from IPython.core.display import HTML #path to .ccs style script cur_path = os . path . dirname ( os . path . abspath ( \"__file__\" )) new_path = os . path . relpath ( '.. \\\\ .. \\\\ styles \\\\ custom_styles_public_cloud_computing.css' , cur_path ) #function to display notebook def css (): style = open ( new_path , \"r\" ) . read () return HTML ( style ) In [ ]: #run this cell to apply HTML style css ()","tags":"Lectures","url":"lectures/lecture1/notebook/"}]}